---
title: "Linear xgboost via FeatureHashing"
author : 'Sravan Roy '
output: html_document
---

```{r Libraries, message=FALSE, warning=FALSE}
library(data.table)
library(FeatureHashing)
library(Matrix)
library(xgboost)
```

### 1) Basic data preparation via data.table

Before merging the people and train tables, we'll convert the True/False people features to 1/0 so that they can be treated as a dummy variable directly later on. The 'set' function is used to map the logical columns to their corresponding integer (1/0). The outcome probability values are separated from the training features for convenience.

```{r data preparation}
people <- fread("people.csv", showProgress = F)
p_logi <- names(people)[which(sapply(people, is.logical))]

for (col in p_logi) set(people, j = col,
                        value = as.integer(people[[col]]))

train  <- fread("act_train.csv", showProgress = F)
d1     <- merge(train, people, by = "people_id", all.x = T)

Y <- d1$outcome
d1[ , outcome := NULL]
```


### 2) Process categorical features via FeatureHashing

The FeatureHashing package is a quick way to encode the categorical features into a sparse matrix. Numeric features will automatically be included in the matrix as they are. We'll exclude the date features along with the id features as they do not  represent any relevant activity info. The performance of the model doesn't appear to improve much above a hash size of 2 ^ 22.

```{r FeatureHashing}
b <- 2 ^ 22
f <- ~ . - people_id - activity_id - date.x - date.y - 1

X_train <- hashed.model.matrix(f, d1, hash.size = b)
```

We can easily check how many columns of the sparse matrix are occupied by at least one row of the training data.

```{r Matrix}
sum(colSums(X_train) > 0)
```


### 3) Validate xgboost model

The linear mode of xgboost provides a good baseline and is faster compared to the time consuming algorithms like boosted trees. The validation set is chosen by people_id.

```{r Validation}
set.seed(123)
unique_p <- unique(d1$people_id)
valid_p  <- unique_p[sample(1:length(unique_p), 30000)]

valid <- which(d1$people_id %in% valid_p)
model <- (1:length(d1$people_id))[-valid]

param <- list(objective = "binary:logistic",
              eval_metric = "auc",
              booster = "gblinear",
              eta = 0.03)

dmodel  <- xgb.DMatrix(X_train[model, ], label = Y[model])
dvalid  <- xgb.DMatrix(X_train[valid, ], label = Y[valid])

m1 <- xgb.train(data = dmodel, param, nrounds = 100,
                watchlist = list(model = dmodel, valid = dvalid),
                print_every_n = 10)

```


### 4) Retrain on all data and predict for test set

We'll watch the training error just to check nothing has gone weird. Another advantage of hashing is that we can process the test data independent of the training data.

```{r Production}
dtrain  <- xgb.DMatrix(X_train, label = Y)

m2 <- xgb.train(data = dtrain, param, nrounds = 100,
                watchlist = list(train = dtrain),
                print_every_n = 10)

test <- fread("act_test.csv", showProgress = F)
d2   <- merge(test, people, by = "people_id", all.x = T)

X_test <- hashed.model.matrix(f, d2, hash.size = b)
dtest  <- xgb.DMatrix(X_test)

out <- predict(m2, dtest)
sub <- data.frame(activity_id = d2$activity_id, outcome = out)
write.csv(sub, file = "sub.csv", row.names = F)
```



